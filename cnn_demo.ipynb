{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CNN Training and Inference Guide\n",
        "\n",
        "**Check Your Dataset Format!!**\n",
        "\n",
        "Before you begin, refer to the readme to ensure your dataset follows the required format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Importing Libraries "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import os\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Dataset Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Set the path to your dataset\n",
        "train_data_path = r\"path_to_your_dataset/train_images\"\n",
        "train_labels_path = r\"path_to_your_dataset/train_labels.csv\"\n",
        "\n",
        "test_data_path = r\"path_to_your_dataset/test_images\"\n",
        "test_labels_path = r\"path_to_your_dataset/test_labels.csv\"\n",
        "\n",
        "# Set to none of you have no validation data, otherwise set the path\n",
        "val_data_path = None\n",
        "val_labels_path = None\n",
        "\n",
        "# Set size to your current image shape, or the shape you want your images resized at\n",
        "img_rows, img_cols = 64, 64\n",
        "\n",
        "# Select 1 for grayscale and 3 for RGB images\n",
        "channels = 3\n",
        "\n",
        "# Folder name where trained model and training results will be saved\n",
        "run_results_folder = \"cnn_run_results\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Processing and Loading Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Load one single image(helper)\n",
        "def load_one(path, data_path, size, channels):\n",
        "    full_path = os.path.join(data_path, path)\n",
        "    if channels == 1:\n",
        "        img = cv2.imread(full_path, cv2.IMREAD_GRAYSCALE)\n",
        "    else:\n",
        "        img = cv2.imread(full_path, cv2.IMREAD_COLOR)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    img = cv2.resize(img, (size[1], size[0]))\n",
        "    return np.array(img, dtype=\"float32\") / 255.0\n",
        "\n",
        "# Load dataset(helper)\n",
        "def process_dataset(data_path, labels_path=None, size=(28, 28), channels=1):\n",
        "    \n",
        "    # Load the labels into Data Frame\n",
        "    df = pd.read_csv(labels_path)\n",
        "\n",
        "    img_id_col = df.columns[0]\n",
        "    files = df[img_id_col].astype(str).tolist()\n",
        "\n",
        "    # Load each image in the dataset and put in one array\n",
        "    images = np.array([load_one(f, data_path, size, channels) for f in files])\n",
        "\n",
        "    # Return X(images) and Y(labels)\n",
        "    if len(df.columns) == 1:                   \n",
        "        return images, None\n",
        "    else:\n",
        "        return images, df[df.columns[1]].values  \n",
        "\n",
        "print(\"Processing datasets...\")\n",
        "\n",
        "# Load datasets\n",
        "X_train, Y_train = process_dataset(train_data_path, train_labels_path, (img_rows, img_cols), channels)\n",
        "X_test, Y_test = process_dataset(test_data_path, test_labels_path, (img_rows, img_cols), channels)\n",
        "if val_data_path:\n",
        "    X_val, Y_val = process_dataset(val_data_path, val_labels_path, (img_rows, img_cols), channels)\n",
        "else :\n",
        "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "# Get the class labels\n",
        "unique_labels = np.unique(Y_train)\n",
        "class_labels = [str(label) for label in sorted(unique_labels)]\n",
        "print(f\"Automatically detected {len(class_labels)} classes: {class_labels}\")\n",
        "\n",
        "# Convert labels to integer indices \n",
        "label_to_idx = {str(lbl): idx for idx, lbl in enumerate(class_labels)}\n",
        "Y_train = np.array([label_to_idx[str(l)] for l in Y_train])\n",
        "Y_val = np.array([label_to_idx[str(l)] for l in Y_val])\n",
        "if Y_test is not None:\n",
        "    Y_test = np.array([label_to_idx[str(l)] for l in Y_test])\n",
        "\n",
        "\n",
        "# Print out the shapes\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"Y_train shape: {Y_train.shape}\")\n",
        "print(f\"X_val shape:   {X_val.shape}\")\n",
        "print(f\"Y_val shape:   {Y_val.shape}\")\n",
        "print(f\"X_test shape:  {X_test.shape}\")\n",
        "print(f\"Y_test shape:  {Y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Visualizing Training Samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "8902b0312e6c047596cf27ebba554a68b82604b2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Plot 6 random samples from the training set\n",
        "if X_train is not None and len(X_train) > 0:\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    indices = np.random.choice(len(X_train), 6, replace=False)\n",
        "    for i, idx in enumerate(indices):\n",
        "        plt.subplot(2, 3, i + 1)\n",
        "        img = X_train[idx]\n",
        "        plt.imshow(img, cmap='gray' if channels == 1 else None)\n",
        "        plt.title(f\"Label: {class_labels[int(Y_train[idx])]}\")\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Label Encoding (One-Hot Encoding)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "d15d35ca439dce194a96f4442c7a1c085ce24d28",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "Y_train = to_categorical(Y_train, num_classes=len(class_labels))\n",
        "Y_val = to_categorical(Y_val, num_classes=len(class_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Defining the CNN Architecture\n",
        "\n",
        "**Customize Your CNN Architecture:**\n",
        "\n",
        "You can modify the architecture by adding, removing, or changing layers:\n",
        "\n",
        "1. **Convolutional Layers (`Conv2D`)**: \n",
        "   - Add more `Conv2D` layers to increase model depth\n",
        "   - Adjust `filters` to control the number of feature maps\n",
        "   - Change `kernel_size` to adjust the receptive field\n",
        "   - Modify `padding` ('Same' or 'valid') to control output size\n",
        "   - Change `activation` function (e.g., 'relu', 'tanh', 'sigmoid')\n",
        "\n",
        "2. **Pooling Layers (`MaxPool2D` or `AvgPool2D`)**:\n",
        "   - Add pooling layers after convolutional layers to reduce spatial dimensions\n",
        "   - Adjust `pool_size` to control downsampling\n",
        "   - Modify `strides` to control the step size\n",
        "\n",
        "3. **Dropout Layers**:\n",
        "   - Add `Dropout` layers to prevent overfitting\n",
        "   - Adjust the dropout rate - higher values mean more regularization\n",
        "\n",
        "4. **Dense (Fully Connected) Layers**:\n",
        "   - Add more `Dense` layers before the final output layer\n",
        "   - Adjust the number of units (e.g., 128, 256, 512) to control model capacity\n",
        "   - The final `Dense` layer must have `len(class_labels)` units with `softmax` activation for classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "c441d7b3852cee5d3636272d4da2f96b169f81ac",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Conv Layer 1\n",
        "model.add(Conv2D(filters = 8, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu', input_shape = (img_rows, img_cols, channels)))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Conv Layer 2\n",
        "model.add(Conv2D(filters = 16, kernel_size = (3,3),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Conv Layer 3\n",
        "model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "# fully connected\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation = \"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(class_labels), activation = \"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Configuring the Optimizer\n",
        "\n",
        "**Choose Your Optimizer and Learning Rate:**\n",
        "\n",
        "You can select from different optimizer functions and adjust the learning rate:\n",
        "\n",
        "1. **Adam Optimizer**:\n",
        "   ```python\n",
        "   optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
        "   ```\n",
        "   - Good default choice, adapts learning rate per parameter\n",
        "\n",
        "2. **RMSprop Optimizer**:\n",
        "   ```python\n",
        "   optimizer = RMSprop(learning_rate=0.001, rho=0.9)\n",
        "   ```\n",
        "   - Good for recurrent networks and non-stationary objectives\n",
        "\n",
        "3. **SGD (Stochastic Gradient Descent)**:\n",
        "   ```python\n",
        "   optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "   ```\n",
        "   - Classic optimizer, can work well with proper learning rate scheduling\n",
        "\n",
        "**Note:**\n",
        "- If training is unstable (loss explodes), reduce learning rate (e.g., 0.0001)\n",
        "- If training is too slow, increase learning rate (e.g., 0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "607a02b42636e3115a1ac7a8edcadf61cf5ea1b0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Compiling the Model\n",
        "\n",
        "**Choose Your Loss Function:**\n",
        "\n",
        "Select an appropriate loss function based on your problem type:\n",
        "\n",
        "1. **`categorical_crossentropy`**:\n",
        "   - Use when you have multiple classes and one-hot encoded labels\n",
        "\n",
        "2. **`sparse_categorical_crossentropy`**:\n",
        "   - Use when you have multiple classes but integer labels\n",
        "\n",
        "3. **`binary_crossentropy`**:\n",
        "   - Use for binary classification problems (2 classes)\n",
        "\n",
        "**Note:** For this demo, since we're using one-hot encoded labels, `categorical_crossentropy` is the default choice. If you change your label encoding method, make sure to update the loss function accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "0d1eefc68470b4cdcec04c2570651da3d97676d0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Setting Training Hyperparameters\n",
        "\n",
        "**Configure Training Parameters:**\n",
        "\n",
        "1. **Epochs**: \n",
        "   - Number of complete passes through the training dataset\n",
        "   - More epochs = longer training time but potentially better accuracy\n",
        "\n",
        "2. **Batch Size**:\n",
        "   - Number of samples processed before the model is updated\n",
        "   - **Larger batch size**:\n",
        "     - Faster training per epoch\n",
        "     - More stable gradients\n",
        "     - Requires more memory\n",
        "   - **Smaller batch size**:\n",
        "     - More frequent updates\n",
        "     - Can help escape local minima\n",
        "     - Less memory required\n",
        "\n",
        "**Note:**\n",
        "- Increasing number of epochs doesn't always equal better results, it \n",
        "might cause overfit \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "a237feb5e53ecbc8799101cb6e699877faafde77",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "batch_size = 250"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Training the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "4b2957bb8976a25cdbbbdc3110d68c5035a9773c",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "history = model.fit(X_train, Y_train, \n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs, \n",
        "                    validation_data=(X_val, Y_val),\n",
        "                    verbose=1)\n",
        "\n",
        "# Save model and results to the folder \n",
        "os.makedirs(run_results_folder, exist_ok=True)\n",
        "model.save(os.path.join(run_results_folder, \"model.keras\"))\n",
        "with open(os.path.join(run_results_folder, \"history.json\"), \"w\") as f:\n",
        "    json.dump(history.history, f, indent=2)\n",
        "with open(os.path.join(run_results_folder, \"class_labels.json\"), \"w\") as f:\n",
        "    json.dump(class_labels, f, indent=2)\n",
        "    \n",
        "print(f\"Model and results saved to '{run_results_folder}' (model.keras, history.json, class_labels.json).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Visualizing Training Loss and Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "180a06f7ae01e69117c6c8258411cfe1b9b7b991",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Load model results and history\n",
        "history_path = os.path.join(run_results_folder, \"history.json\")\n",
        "with open(history_path) as f:\n",
        "    hist_dict = json.load(f)\n",
        "class _History:\n",
        "    pass\n",
        "history = _History()\n",
        "history.history = hist_dict\n",
        "\n",
        "# Plot the loss and accuracy curves for training and validation \n",
        "plt.plot(history.history['val_loss'], color='b', label=\"validation loss\")\n",
        "plt.title(\"Test Loss\")\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Model Evaluation using Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "6586e37bd470db822086e191a90388e7175d504f",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Load model and labels\n",
        "model_path = os.path.join(run_results_folder, \"model.keras\")\n",
        "model = keras.models.load_model(model_path)\n",
        "labels_path = os.path.join(run_results_folder, \"class_labels.json\")\n",
        "with open(labels_path) as f:\n",
        "    class_labels = json.load(f)\n",
        "\n",
        "# Predict the values from the validation dataset\n",
        "Y_pred = model.predict(X_val)\n",
        "# Convert predictions classes to one hot vectors \n",
        "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(Y_val,axis = 1) \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
        "# plot the confusion matrix\n",
        "f,ax = plt.subplots(figsize=(8, 8))\n",
        "sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Final Predictions and Visualization on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Predict and visualize 6 random samples from the test set\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Select 6 random images from test data\n",
        "indices = np.random.choice(len(X_test), 6, replace=False)\n",
        "selected_test = X_test[indices]\n",
        "\n",
        "# Make the model predit the images\n",
        "predictions = model.predict(selected_test)\n",
        "pred_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Get true labels for selected indices if available\n",
        "true_classes = Y_test[indices] if Y_test is not None else None\n",
        "\n",
        "# Plot\n",
        "for i, idx in enumerate(indices):\n",
        "    plt.subplot(2, 3, i + 1)\n",
        "    img = X_test[idx]\n",
        "    plt.imshow(img, cmap='gray' if channels == 1 else None)\n",
        "    \n",
        "    # Show both predicted and true labels if available\n",
        "    if true_classes is not None:\n",
        "        true_label = class_labels[true_classes[i]]\n",
        "        pred_label = class_labels[pred_classes[i]]\n",
        "        color = 'green' if true_classes[i] == pred_classes[i] else 'red'\n",
        "        plt.title(f\"Predicted: {pred_label}\\nTrue: {true_label}\", color=color)\n",
        "    else:\n",
        "        plt.title(f\"Predicted: {class_labels[pred_classes[i]]}\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Citations\n",
        "\n",
        "Based on \"Convolutional Neural Network (CNN) Tutorial\" by [kanncaa1](https://www.kaggle.com/kanncaa1), licensed under the Apache License 2.0. Modifications have been made.\n"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dockerImageVersionId": 9432,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "venv_cvdemo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
